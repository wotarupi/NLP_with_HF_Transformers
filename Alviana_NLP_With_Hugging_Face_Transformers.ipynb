{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CztodpmQM11q"
      },
      "source": [
        "# Let's try HuggingFace Transformers NLP Pipelines!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dtAhiTAMmYN",
        "outputId": "777cd930-ed2c-43c5-b0b5-feed26ac30ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3g0V_WTMpwB",
        "outputId": "7af28976-0a1a-4287-bf6f-61471c42666a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Teknologi', 0.4930022358894348), ('Hobi', 0.31904467940330505)]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the Zero-Shot Classification pipeline with facebook/bart model\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Contoh teks dalam bahasa Indonesia (mencoba dalam bahasa indonesia karena katanya bisa tetapi tidak se akurat saat memakai indobert)\n",
        "text = \"Saya suka belajar AI dan programming.\"\n",
        "\n",
        "# Daftar label yang ingin diuji\n",
        "labels = [\"Teknologi\", \"Olahraga\", \"Hobi\"]\n",
        "\n",
        "# Jalankan klasifikasi\n",
        "threshold = 0.2  # Ambang batas 20%\n",
        "relevant_labels = [\n",
        "    (label, score) for label, score in zip(result['labels'], result['scores']) if score > threshold\n",
        "]\n",
        "print(relevant_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qwkn1wZaNCxn",
        "outputId": "50fac2ff-21e0-4c00-9582-8d711db65d27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"I want to break up with you because you think of you as a child. You don't know what you're talking about, you don't care\"}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load GPT-2 for text generation in Bahasa Indonesia\n",
        "text_generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# Prompt awal\n",
        "prompt = \"I want to break up with you because\"\n",
        "\n",
        "# Hasil text generation\n",
        "text_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "output = text_generator(prompt, max_length=30, truncation=True, num_return_sequences=1)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ab97He7Dd0k",
        "outputId": "923d2d8d-9f07-4ee2-a7a0-9bec32d0a05b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.09807245433330536, 'token': 7759, 'token_str': 'dining', 'sequence': 'the teacher is in the dining room.'}, {'score': 0.08875711262226105, 'token': 5059, 'token_str': 'drawing', 'sequence': 'the teacher is in the drawing room.'}, {'score': 0.034135833382606506, 'token': 3564, 'token_str': 'sitting', 'sequence': 'the teacher is in the sitting room.'}, {'score': 0.021684743463993073, 'token': 8835, 'token_str': 'lecture', 'sequence': 'the teacher is in the lecture room.'}, {'score': 0.02137482352554798, 'token': 2364, 'token_str': 'main', 'sequence': 'the teacher is in the main room.'}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load BertForMaskedLM with trust_remote_code=True (jika diperlukan)\n",
        "fill_mask = pipeline(\"fill-mask\", model=\"distilbert-base-uncased\", trust_remote_code=True)\n",
        "\n",
        "# Contoh input\n",
        "output = fill_mask(\"the teacher is in the [MASK] room.\")\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-uxwPRKDhpB",
        "outputId": "821ea3fd-2d03-4d43-f024-b5c4ffdd770e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'entity': 'LABEL_1', 'score': 0.52284026, 'index': 1, 'word': 'john', 'start': 0, 'end': 4}, {'entity': 'LABEL_0', 'score': 0.53089684, 'index': 2, 'word': 'doe', 'start': 5, 'end': 8}, {'entity': 'LABEL_0', 'score': 0.5286107, 'index': 3, 'word': 'works', 'start': 9, 'end': 14}, {'entity': 'LABEL_0', 'score': 0.5016301, 'index': 4, 'word': 'at', 'start': 15, 'end': 17}, {'entity': 'LABEL_1', 'score': 0.5062397, 'index': 5, 'word': 'google', 'start': 18, 'end': 24}, {'entity': 'LABEL_0', 'score': 0.5087919, 'index': 6, 'word': 'in', 'start': 25, 'end': 27}, {'entity': 'LABEL_1', 'score': 0.53328663, 'index': 7, 'word': 'san', 'start': 28, 'end': 31}, {'entity': 'LABEL_1', 'score': 0.5620234, 'index': 8, 'word': 'francisco', 'start': 32, 'end': 41}, {'entity': 'LABEL_1', 'score': 0.5593502, 'index': 9, 'word': 'on', 'start': 42, 'end': 44}, {'entity': 'LABEL_1', 'score': 0.5206075, 'index': 10, 'word': 'december', 'start': 45, 'end': 53}, {'entity': 'LABEL_1', 'score': 0.61758155, 'index': 11, 'word': '18', 'start': 54, 'end': 56}, {'entity': 'LABEL_0', 'score': 0.58020097, 'index': 12, 'word': ',', 'start': 56, 'end': 57}, {'entity': 'LABEL_0', 'score': 0.5284733, 'index': 13, 'word': '202', 'start': 58, 'end': 61}, {'entity': 'LABEL_1', 'score': 0.5259489, 'index': 14, 'word': '##4', 'start': 61, 'end': 62}, {'entity': 'LABEL_1', 'score': 0.54327357, 'index': 15, 'word': '.', 'start': 62, 'end': 63}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load NER pipeline\n",
        "ner_pipeline = pipeline(\"ner\", model=\"distilbert-base-uncased\")\n",
        "\n",
        "# Contoh input dalam bahasa Inggris\n",
        "text = \"John Doe works at Google in San Francisco on December 18, 2024.\"\n",
        "\n",
        "# Hasil NER\n",
        "output = ner_pipeline(text)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SbkDlwfDlF5",
        "outputId": "ef8ac9bb-cfd2-4f65-cbf9-ec4f60573afc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-cased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jawaban 1: AI adalah teknologi yang dapat meniru kecerdasan manusia (Skor: 0.048151470720767975)\n",
            "Jawaban 2: Teknologi ini digunakan untuk menyelesaikan masalah (Skor: 0.025670038536190987)\n",
            "Jawaban 3: AI adalah teknologi yang dapat meniru kecerdasan manusia. (Skor: 0.024041395634412766)\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"bert-large-cased-whole-word-masking-finetuned-squad\")\n",
        "context = \"AI adalah teknologi yang dapat meniru kecerdasan manusia. Teknologi ini digunakan untuk menyelesaikan masalah yang biasanya memerlukan kecerdasan manusia.\"\n",
        "question = \"Apa definisi AI dalam kalimat ini?\"\n",
        "\n",
        "result = qa_pipeline(question=question, context=context, max_answer_len=30, top_k=3)\n",
        "for i, answer in enumerate(result):\n",
        "    print(f\"Jawaban {i+1}: {answer['answer']} (Skor: {answer['score']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3IZcIeAE41r",
        "outputId": "03547630-c0b0-4a48-f7fb-4ded552bd683"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'positive', 'score': 0.998643696308136}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"w11wo/indonesian-roberta-base-sentiment-classifier\")\n",
        "\n",
        "# Input kalimat\n",
        "text = \"Saya sangat puas dengan produk ini!\"\n",
        "\n",
        "# Analisis sentimen\n",
        "result = sentiment_pipeline(text)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXSjHIImDptH",
        "outputId": "3771a9b7-20b4-49b6-a609-61c95aee18d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kecerdasan buatan( AI) adalah teknologi yang semakin berkembang pesat dalam beberapa dekade terakhir. Teknologi ini memiliki kemampuan untuk meniru pola pikir manusia dan memproses data dalam jumlah besar untuk menghasilkan keputusan yang lebih akurat. Saat ini, AI telah diterapkan di berbagai sektor, mulai dari kesehatan, transportasi, pendidikan, hingga hiburan. Dalam bidang kesehatan, AI digunakan untuk mendiagnosis penyakit dengan cepat dan akurat, seperti menganalisis hasil pemindaian medis atau mencocokkan gejala pasien dengan basis data penyakit. Di\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Inisialisasi pipeline untuk summarization\n",
        "summarization_pipeline = pipeline(\"summarization\", model=\"cahya/t5-base-indonesian-summarization-cased\")\n",
        "\n",
        "# Teks yang lebih panjang untuk dirangkum\n",
        "text = \"\"\"\n",
        "Kecerdasan buatan (AI) adalah teknologi yang semakin berkembang pesat dalam beberapa dekade terakhir.\n",
        "Teknologi ini memiliki kemampuan untuk meniru pola pikir manusia dan memproses data dalam jumlah besar\n",
        "untuk menghasilkan keputusan yang lebih akurat. Saat ini, AI telah diterapkan di berbagai sektor, mulai dari kesehatan,\n",
        "transportasi, pendidikan, hingga hiburan. Dalam bidang kesehatan, AI digunakan untuk mendiagnosis penyakit dengan cepat dan akurat,\n",
        "seperti menganalisis hasil pemindaian medis atau mencocokkan gejala pasien dengan basis data penyakit.\n",
        "Di sektor transportasi, AI membantu menciptakan kendaraan otonom yang mampu mengemudi tanpa campur tangan manusia,\n",
        "yang diharapkan dapat mengurangi kecelakaan lalu lintas. Selain itu, dalam dunia pendidikan, AI digunakan untuk memberikan\n",
        "pengalaman belajar yang lebih personal melalui sistem pembelajaran adaptif yang dapat menyesuaikan materi sesuai dengan\n",
        "kebutuhan masing-masing siswa. Terakhir, AI juga memainkan peran penting dalam industri hiburan, seperti menciptakan rekomendasi\n",
        "konten yang sesuai dengan preferensi pengguna di platform streaming. Meskipun AI menawarkan banyak manfaat, teknologi ini juga\n",
        "menimbulkan kekhawatiran, seperti masalah privasi, etika, dan dampaknya terhadap lapangan pekerjaan. Oleh karena itu, penting untuk\n",
        "mengembangkan kebijakan yang bertanggung jawab dalam penerapan AI agar manfaatnya dapat dirasakan secara luas tanpa mengabaikan\n",
        "potensi risiko yang mungkin timbul.\n",
        "\"\"\"\n",
        "\n",
        "# Melakukan summarization\n",
        "summary = summarization_pipeline(\n",
        "    text,\n",
        "    max_length=97,\n",
        "    min_length=40,\n",
        "    do_sample=False,\n",
        "    no_repeat_ngram_size=3  # Mencegah pengulangan n-gram\n",
        ")\n",
        "\n",
        "print(summary[0]['summary_text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wxr6a3YYDtmL",
        "outputId": "31341ecf-4861-4905-f080-32a851d85ce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indo ke Inggris: Sun\n",
            "Inggris ke Jepang: キ\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Indo ke Inggris\n",
        "translator_id_to_en = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-id-en\")\n",
        "id_to_en = translator_id_to_en(\"Matahari\")\n",
        "print(\"Indo ke Inggris:\", id_to_en[0]['translation_text'])\n",
        "\n",
        "# Inggris ke Jepang\n",
        "translator_en_to_jp = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-jap\")\n",
        "en_to_jp = translator_en_to_jp(id_to_en[0]['translation_text'])\n",
        "print(\"Inggris ke Jepang:\", en_to_jp[0]['translation_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ANALISIS**"
      ],
      "metadata": {
        "id": "8BQWJO2YQu_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ada beberapa yang sudah saya coba seperti:\n",
        "1. zero-shot-classification\n",
        "\n",
        "```\n",
        "-> untuk klasifikasi kalimat\n",
        "model: facebook/bart-large-mnli\n",
        "karena katanya bisa pakai bahasa indonesia tapi tidak seakurat model indobert tetapi pas di coba lumayan bisa dipakai\n",
        "```\n",
        "2. text-generation\n",
        "\n",
        "\n",
        "```\n",
        "-> untuk generate text agar lebih panjang\n",
        "model: gpt2\n",
        "Model utama yang kuat dan sering digunakan untuk text generation\n",
        "```\n",
        "\n",
        "\n",
        "3. fill-mask\n",
        "\n",
        "```\n",
        "->mengisi kata yang kosong dalam sebuah kalimat\n",
        "model: distilbert-base-uncased\n",
        "trust_remote_code=True\n",
        "setelah saya nanya chatgpt, sebelum saya pakai ini ada sebuah error dan error itu karena BertForMaskedLM tidak secara langsung mewarisi GenerationMixin.\n",
        "Beberapa bobot model mungkin tidak cocok jika digunakan di luar arsitektur aslinya (misalnya, dari model bert yang dirancang untuk tugas lain seperti sequence classification) dan fungsi parameter ini untuk mengabaikan error tersebut dan sudah bisa dijalankan\n",
        "```\n",
        "\n",
        "\n",
        "4. ner\n",
        "\n",
        "```\n",
        "-> untuk mengidentifikasi dan mengekstraksi entitas yang diberi nama dalam teks, seperti nama orang, tempat, organisasi, tanggal, jumlah, dll. (chatgpt)\n",
        "model: distilbert-base-uncased\n",
        "model ini memang sudah terlatih untuk ner\n",
        "```\n",
        "\n",
        "5. question-answering\n",
        "\n",
        "```\n",
        "-> untuk menjawab sesuai dengan statement yang sudah diberikan\n",
        "model: bert-large-cased-whole-word-masking-finetuned-squad\n",
        "awalnya mau pake bert-base-cased tetapi yang diambil kalimatnya hanya pertengahan bukan seluruh pengertian, setelah cari-cari luntang-lantung begitu nemu model ini dan berhasil\n",
        "```\n",
        "\n",
        "6. sentiment-analysis\n",
        "\n",
        "```\n",
        "-> menentukan apakah ini kalimat positif atau negatif atau netral\n",
        "model: w11wo/indonesian-roberta-base-sentiment-classifier\n",
        "model ini dilatih untuk sentiment analysis versi bahasa indonesia jadi lebih masuk akal dibandingkan dengan yang umum kayak distilbert\n",
        "```\n",
        "\n",
        "7. summarization\n",
        "\n",
        "```\n",
        "-> seperti namanya adalah untuk menyimpulkan teks\n",
        "model: cahya/t5-base-indonesian-summarization-cased\n",
        "model ini sengaja saya pakai biar bisa dipakai untuk teks berbahasa indonesia\n",
        "```\n",
        "\n",
        "8. translation\n",
        "\n",
        "```\n",
        "-> untuk menerjemahkan teks atau kata\n",
        "model: Helsinki-NLP/opus-mt-id-en\n",
        "model: Helsinki-NLP/opus-mt-en-jap\n",
        "disini saya memakai 2 model karena tujuan awalnya untuk mengubah dari bahasa indonesia ke bahasa jepang dan ternyata tidak ada model yang langsung seperti itu. Jadi saya pakai dari bahasa indonesia diterjemahin ke bahasa inggris dulu, nanti bahasa inggris ini yang akan diterjemahkan ke bahasa jepang. walaupun yang dari bahasa inggris ke jepang masih belum berhasil karena agak lari terjemahannya, setidaknya sudah bisa ke bahasa jepang dulu.\n",
        "```\n",
        "\n",
        "Dan dari semua ini, saya masih penasaran dengan translation karena unik dan masih penasaran kenapa bahasa jepang nya masih lari terjemahannya, mungkin kedepannya saya akan mencoba explore lebih jauh lagi tentang ini.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CwDZbCgBQzgE"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}